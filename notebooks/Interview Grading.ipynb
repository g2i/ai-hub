{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "8cd19172-ea0d-4079-8452-2f20a95a0e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Annotated, Optional, List, Literal\n",
    "from langchain_core.tools import tool\n",
    "from typing_extensions import TypedDict\n",
    "from datetime import datetime\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from pydantic import BaseModel, Field\n",
    "import requests\n",
    "from rich import print_json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from langgraph.types import Command, interrupt\n",
    "from IPython.display import Image, display\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "204f1624-b299-4cc7-b04d-2a14142672f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_DISABLE_GRAPH_VIZ\"] = \"true\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "model = init_chat_model(\"gpt-4.1-nano\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "34638b77-514d-4002-9bae-5085e9ee08df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Literal, Dict, Any\n",
    "\n",
    "Score = Annotated[int, Field(ge=0, le=100)]\n",
    "\n",
    "class Diarization(BaseModel):\n",
    "    speaker0: Optional[str]\n",
    "    speaker1: Optional[str]\n",
    "\n",
    "\n",
    "class VideoAttributes(BaseModel):\n",
    "    createdAt: datetime\n",
    "    updatedAt: Optional[datetime]\n",
    "    applicationId: Optional[str]\n",
    "    url: Optional[str]\n",
    "    playbackId: Optional[str]\n",
    "    assetId: Optional[str]\n",
    "    duration: float\n",
    "    isActive: bool\n",
    "    question: Optional[str]\n",
    "    signedUrl: Optional[str]\n",
    "    transcript: Optional[str]\n",
    "    jobId: Optional[str]\n",
    "    source: Optional[str]\n",
    "    diarization: Optional[Diarization]\n",
    "    summary: Optional[str]\n",
    "    description: Optional[str]\n",
    "    developerId: Optional[str]\n",
    "\n",
    "\n",
    "class Video(BaseModel):\n",
    "    id: str\n",
    "    type: str\n",
    "    attributes: VideoAttributes\n",
    "\n",
    "# Structured output model for interview Q&A pairs\n",
    "class QuestionAnswerPair(BaseModel):\n",
    "    \"\"\"A pair of question and answer from the interview.\"\"\"\n",
    "    \n",
    "    question_text: str = Field(description=\"The full text of the question asked by the interviewer\")\n",
    "    answer_text: str = Field(description=\"The full text of the answer given by the interviewee\")\n",
    "    rating: Optional[str] = Field(description=\"Rating of the answer (Strong Yes, Yes, No, Strong No)\", default=\"\")\n",
    "    score: Optional[int] = Field(description=\"Score for this answer on a scale of 0-100\", ge=0, le=100, default=0)\n",
    "    \n",
    "class QuestionAnswerPairs(BaseModel):\n",
    "    \"\"\"Collection of question-answer pairs from an interview.\"\"\"\n",
    "    qa_pairs: List[QuestionAnswerPair]\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    score: Score = 0\n",
    "    video: Optional[Video] = None\n",
    "    qa_pairs: List[QuestionAnswerPair] = Field(default_factory=list)\n",
    "    url: str\n",
    "    error: Optional[str] = \"\"\n",
    "    speaker_identification: Optional[Dict[str, str]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "8b4f3c5e-15c9-4f42-9d22-590ed70809b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_interview_state(state: Evaluation):\n",
    "    # Access the URL directly from the Evaluation object\n",
    "    video_url = state.url\n",
    "    video = requests.get(video_url, headers={'authorization': 'Bearer cd6f3a3b-7cb5-43f7-a332-dd52c0b39e1c'})\n",
    "    # Return the video data to update state\n",
    "    return {\"video\": video.json().get('data')}\n",
    "\n",
    "async def identify_speakers(state: Evaluation):\n",
    "    print(\"Processing speakers from video data\")\n",
    "    try:\n",
    "        # Extract transcript\n",
    "        transcript = state.video.attributes.transcript\n",
    "        \n",
    "        if not transcript or not transcript.strip():\n",
    "            return {\"error\": \"No transcript available for speaker identification\"}\n",
    "        \n",
    "        # Define JSON Schema for speaker identification\n",
    "        speaker_schema = {\n",
    "            \"title\": \"SpeakerIdentification\",\n",
    "            \"description\": \"Identification of speakers in an interview transcript\",\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"interviewer\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Label for the interviewer (speaker0 or speaker1)\"\n",
    "                },\n",
    "                \"interviewee\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Label for the interviewee (speaker0 or speaker1)\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"interviewer\", \"interviewee\"]\n",
    "        }\n",
    "        \n",
    "        # Create messages for speaker identification\n",
    "        messages = [\n",
    "            SystemMessage(content=\"\"\"\n",
    "                You are an AI assistant specialized in analyzing technical interview transcripts.\n",
    "                \n",
    "                Your task is to identify which speaker is the interviewer and which is the interviewee in the provided transcript.\n",
    "                \n",
    "                The interviewer typically:\n",
    "                - Asks most of the questions\n",
    "                - Guides the conversation\n",
    "                - Introduces coding problems or technical concepts\n",
    "                - Evaluates responses\n",
    "                \n",
    "                The interviewee typically:\n",
    "                - Answers questions\n",
    "                - Explains their reasoning\n",
    "                - Provides solutions to coding problems\n",
    "                - Demonstrates technical knowledge\n",
    "                \n",
    "                Return your analysis with the labels \"interviewer\" and \"interviewee\" assigned to either \"speaker0\" or \"speaker1\".\n",
    "            \"\"\"),\n",
    "            HumanMessage(content=f\"Here is the interview transcript:\\n\\n{transcript}\")\n",
    "        ]\n",
    "        \n",
    "        # Use structured output with our speaker schema\n",
    "        structured_llm = model.with_structured_output(speaker_schema)\n",
    "        \n",
    "        # Get response and extract speaker roles\n",
    "        try:\n",
    "            response = await structured_llm.ainvoke(messages)\n",
    "            print(f\"Identified speakers: Interviewer={response['interviewer']}, Interviewee={response['interviewee']}\")\n",
    "            \n",
    "            # Simply return the speaker roles - LangGraph will handle state updates\n",
    "            return {\"speaker_identification\": response}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error identifying speakers: {e}\")\n",
    "            return {\"error\": f\"Error identifying speakers: {e}\"}\n",
    "            \n",
    "    except AttributeError as e:\n",
    "        print(f\"Error accessing transcript: {e}\")\n",
    "        return {\"error\": \"Could not access transcript for speaker identification\"}"
   ]
  },
  {
   "cell_type": "code",
   "id": "fcd13f14-72b1-4f80-82b8-6598a39f925c",
   "metadata": {},
   "outputs": [],
   "source": "async def extract_questions(state: Evaluation):\n    print(\"Extracting detailed question-answer pairs from interview transcript\")\n    try:\n        transcript = state.video.attributes.transcript\n        \n        if not transcript or not transcript.strip():\n            return {\"error\": \"No transcript\", \"qa_pairs\": []}\n        \n        # Get speaker identification if available\n        interviewer = getattr(state, 'speaker_identification', {}).get('interviewer')\n        interviewee = getattr(state, 'speaker_identification', {}).get('interviewee')\n        \n        # Using Pydantic v2 approach with RootModel\n        from pydantic import RootModel\n        # Ensure List is imported here\n        from typing import List\n        QAPairsList = RootModel[List[QuestionAnswerPair]]\n        \n        # Build detailed prompt with instructions\n        system_message = f\"\"\"\n        You are analyzing a technical interview transcript to extract detailed question-answer pairs.\n        \n        Guidelines for extraction:\n        1. Identify complete technical questions that test knowledge\n        2. Capture the full context of both questions and answers\n        3. Include any code examples mentioned in questions or answers\n        4. Recognize multi-part questions and answers that span multiple turns\n        5. Focus only on substantial technical questions, not conversational remarks\n        \n        In this transcript:\n        - The interviewer is labeled as {interviewer}\n        - The interviewee is labeled as {interviewee}\n        \n        Extract at least 5 substantial technical questions from the interviewer and full answers from the interviewee.\n        \"\"\"\n        \n        human_message = f\"Here is the interview transcript to analyze:\\n\\n{transcript}\"\n        \n        messages = [\n            SystemMessage(content=system_message),\n            HumanMessage(content=human_message)\n        ]\n        \n        # Alternative approach using messages without structured output\n        response = await model.ainvoke(messages)\n        \n        # Parse the response using a more robust approach\n        import json\n        import re\n        \n        # Process the text response to extract json-like content\n        # This is a fallback approach since structured output failed\n        qa_pairs: List[QuestionAnswerPair] = []\n        \n        # Use a direct prompt that requests specific formatting\n        extraction_prompt = f\"\"\"\n        Analyze this technical interview and extract exactly 5-7 question-answer pairs.\n        The interviewer is {interviewer} and the interviewee is {interviewee}.\n        \n        For each pair, format your response as:\n        \n        QUESTION: [Full question text]\n        ANSWER: [Full answer text]\n        TOPIC: [Technical topic]\n        \n        Make sure to capture multi-part questions and their complete answers.\n        Include relevant code examples in both questions and answers.\n        Focus on substantial technical questions, not conversational remarks.\n        \n        Transcript:\n        {transcript}\n        \"\"\"\n        \n        extraction_response = await model.ainvoke(extraction_prompt)\n        \n        # Process the formatted response\n        response_text = extraction_response.content\n        qa_blocks = re.split(r'QUESTION:', response_text)[1:]  # Skip the first empty element\n        \n        for block in qa_blocks:\n            try:\n                question_text = block.split('ANSWER:')[0].strip()\n                remaining = block.split('ANSWER:')[1]\n                \n                # Handle if TOPIC is present\n                if 'TOPIC:' in remaining:\n                    answer_text = remaining.split('TOPIC:')[0].strip()\n                    # We don't need to store topic but could add it if QuestionAnswerPair is updated\n                else:\n                    answer_text = remaining.strip()\n                \n                qa_pairs.append(QuestionAnswerPair(\n                    question_text=question_text,\n                    answer_text=answer_text\n                ))\n            except Exception as e:\n                print(f\"Error parsing a Q&A block: {e}\")\n        \n        print(f\"Extracted {len(qa_pairs)} detailed question-answer pairs\")\n        return {\"qa_pairs\": qa_pairs}\n        \n    except Exception as e:\n        print(f\"Error extracting question-answer pairs: {e}\")\n        return {\"error\": f\"Error extracting question-answer pairs: {e}\", \"qa_pairs\": []}"
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "b762c2a5-5ec7-49f2-b10c-8e9410af1b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the state graph\n",
    "builder = StateGraph(Evaluation)\n",
    "\n",
    "# Add nodes to the graph\n",
    "builder.add_node(\"get_interview_state\", get_interview_state)\n",
    "builder.add_node(\"identify_speakers\", identify_speakers)\n",
    "builder.add_node(\"extract_questions\", extract_questions)\n",
    "\n",
    "# Connect the nodes in the workflow\n",
    "builder.add_edge(START, \"get_interview_state\")\n",
    "builder.add_edge(\"get_interview_state\", \"identify_speakers\")\n",
    "builder.add_edge(\"identify_speakers\", \"extract_questions\")\n",
    "builder.add_edge(\"extract_questions\", END)\n",
    "\n",
    "# Compile the graph\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "8fc5d9b9-dc2b-4a46-898b-28239e61f576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting interview evaluation...\n",
      "Processing speakers from video data\n",
      "Identified speakers: Interviewer=speaker0, Interviewee=speaker1\n",
      "Extracting detailed question-answer pairs from interview transcript\n",
      "Error extracting question-answer pairs: cannot access local variable 'List' where it is not associated with a value\n",
      "State graph execution completed\n"
     ]
    }
   ],
   "source": [
    "# Initialize the graph with starting data, including a URL\n",
    "evaluation = Evaluation(url=\"https://core.g2i.co/api/v2/videos/250c4ef3-114a-4709-8fdc-3419d48f8908\")\n",
    "\n",
    "# Print log messages during execution\n",
    "print(\"Starting interview evaluation...\")\n",
    "\n",
    "# Run the graph\n",
    "result = await graph.ainvoke(evaluation)\n",
    "print(\"State graph execution completed\")\n",
    "\n",
    "# Print essential info\n",
    "# print(f\"Error status: {'Error: ' + result.error if result.error else 'No errors'}\")\n",
    "# print(f\"Video ID: {result.video.id if result.video else 'No video data'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "717c1f84-c97d-4b06-baf6-d746deaa905c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QuestionAnswerPair(question_text='In this transcript: the interviewer is labeled as speaker0 and the interviewee is labeled as speaker1. Extract questions from speaker0 and answers from speaker1. Extract at least 3 question-answer pairs.', answer_text='The first question was about implementing a function to get active users with cats, specifying the conditions and using the filter method in JavaScript.\\nThe second question asked what kind of argument a certain create new user function takes and to explain the syntax, including default values and spread operator.\\nThe third question was about what is missing inside a class to instantiate an object properly, specifically the constructor.', rating='', score=0), QuestionAnswerPair(question_text='What is missing inside the person class to be able to instantiate this object properly?', answer_text=\"It's missing the constructor for this class. On JavaScript, you need to provide a constructor function to initialize the object. Otherwise, it will use the default constructor, which does not set the properties explicitly.\", rating='', score=0), QuestionAnswerPair(question_text='What happens when this express route handler gets called, and how does the code attempt to handle file reading?', answer_text=\"When the route handler is called, it tries to read a JSON file from the same directory using a callback. If an error occurs during reading, it throws an error, though the try-catch block won't catch errors thrown asynchronously in the callback. If successful, it sends back the file contents as the response.\", rating='', score=0), QuestionAnswerPair(question_text='How can a server handle arbitrary large peaks in traffic based on the traffic pattern graph, and what strategies are suggested?', answer_text='Strategies include optimizing database queries, identifying and optimizing third-party applications, and offloading heavy tasks to background workers (e.g., using a queue system with a worker listening for events). This prevents the server from being overwhelmed during peak traffic times.', rating='', score=0)]\n"
     ]
    }
   ],
   "source": [
    "print(result[\"qa_pairs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914c7c1a-0c8e-4451-b189-7f76d2f51893",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}