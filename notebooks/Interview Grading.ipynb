{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "8cd19172-ea0d-4079-8452-2f20a95a0e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Annotated, Optional, List, Literal\n",
    "from langchain_core.tools import tool\n",
    "from typing_extensions import TypedDict\n",
    "from datetime import datetime\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from pydantic import BaseModel, Field\n",
    "import requests\n",
    "from rich import print_json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from langgraph.types import Command, interrupt\n",
    "from IPython.display import Image, display\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langfuse import Langfuse\n",
    "from langfuse.callback import CallbackHandler\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import asyncio\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "204f1624-b299-4cc7-b04d-2a14142672f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_DISABLE_GRAPH_VIZ\"] = \"true\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Define the chat model initialization function\n",
    "def init_chat_model(model_name, model_provider=\"openai\", temperature=0):\n",
    "    if model_provider == \"openai\":\n",
    "        return ChatOpenAI(\n",
    "            model=model_name,\n",
    "            temperature=temperature\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model provider: {model_provider}\")\n",
    "\n",
    "# Initialize the model\n",
    "model = init_chat_model(\"gpt-4.1-nano\", model_provider=\"openai\")\n",
    "\n",
    "langfuse_handler = CallbackHandler(\n",
    "  secret_key=\"sk-lf-f01d262f-12dd-49a6-8b06-61cae9cf9118\",\n",
    "  public_key=\"pk-lf-d988846f-2f7e-424e-a029-a00a4840b729\",\n",
    "  host=\"https://cloud.langfuse.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "34638b77-514d-4002-9bae-5085e9ee08df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Literal, Dict, Any\n",
    "\n",
    "Score = Annotated[int, Field(ge=0, le=100)]\n",
    "\n",
    "class Diarization(BaseModel):\n",
    "    speaker0: Optional[str]\n",
    "    speaker1: Optional[str]\n",
    "\n",
    "\n",
    "class VideoAttributes(BaseModel):\n",
    "    createdAt: datetime\n",
    "    updatedAt: Optional[datetime]\n",
    "    applicationId: Optional[str]\n",
    "    url: Optional[str]\n",
    "    playbackId: Optional[str]\n",
    "    assetId: Optional[str]\n",
    "    duration: float\n",
    "    isActive: bool\n",
    "    question: Optional[str]\n",
    "    signedUrl: Optional[str]\n",
    "    transcript: Optional[str]\n",
    "    jobId: Optional[str]\n",
    "    source: Optional[str]\n",
    "    diarization: Optional[Diarization]\n",
    "    summary: Optional[str]\n",
    "    description: Optional[str]\n",
    "    developerId: Optional[str]\n",
    "\n",
    "\n",
    "class Video(BaseModel):\n",
    "    id: str\n",
    "    type: str\n",
    "    attributes: VideoAttributes\n",
    "\n",
    "# Define grading details model\n",
    "class GradingDetails(BaseModel):\n",
    "    \"\"\"Detailed evaluation of an interview answer.\"\"\"\n",
    "    score: int = Field(ge=0, le=100, description=\"Score on scale of 0-100\")\n",
    "    rating: str = Field(description=\"Qualitative rating (Strong Yes, Yes, No, Strong No)\")\n",
    "    key_facts: List[str] = Field(description=\"Key technical facts/concepts demonstrated in the answer\")\n",
    "    strengths: List[str] = Field(description=\"Strengths of the answer\")\n",
    "    weaknesses: List[str] = Field(description=\"Weaknesses or areas for improvement in the answer\")\n",
    "    justification: str = Field(description=\"Brief justification for the score and rating\")\n",
    "\n",
    "# Structured output model for interview Q&A pairs\n",
    "class QuestionAnswerPair(BaseModel):\n",
    "    \"\"\"A pair of question and answer from the interview.\"\"\"\n",
    "    \n",
    "    question_text: str = Field(description=\"The full text of the question asked by the interviewer\")\n",
    "    answer_text: str = Field(description=\"The full text of the answer given by the interviewee\")\n",
    "    rating: Optional[str] = Field(description=\"Rating of the answer (Strong Yes, Yes, No, Strong No)\", default=\"\")\n",
    "    score: Optional[int] = Field(description=\"Score for this answer on a scale of 0-100\", ge=0, le=100, default=0)\n",
    "    grading_details: Optional[GradingDetails] = Field(default=None, description=\"Detailed grading information\")\n",
    "    \n",
    "class QuestionAnswerPairs(BaseModel):\n",
    "    \"\"\"Collection of question-answer pairs from an interview.\"\"\"\n",
    "    qa_pairs: List[QuestionAnswerPair]\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    score: Score = 0\n",
    "    video: Optional[Video] = None\n",
    "    qa_pairs: List[QuestionAnswerPair] = Field(default_factory=list)\n",
    "    url: str\n",
    "    error: Optional[str] = \"\"\n",
    "    speaker_identification: Optional[Dict[str, str]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "8b4f3c5e-15c9-4f42-9d22-590ed70809b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_interview_state(state: Evaluation):\n",
    "    # Access the URL directly from the Evaluation object\n",
    "    video_url = state.url\n",
    "    video = requests.get(video_url, headers={'authorization': 'Bearer cd6f3a3b-7cb5-43f7-a332-dd52c0b39e1c'})\n",
    "    # Return the video data to update state\n",
    "    return {\"video\": video.json().get('data')}\n",
    "\n",
    "async def identify_speakers(state: Evaluation):\n",
    "    print(\"Processing speakers from video data\")\n",
    "    try:\n",
    "        # Extract transcript\n",
    "        transcript = state.video.attributes.transcript\n",
    "        \n",
    "        if not transcript or not transcript.strip():\n",
    "            return {\"error\": \"No transcript available for speaker identification\"}\n",
    "        \n",
    "        # Define JSON Schema for speaker identification\n",
    "        speaker_schema = {\n",
    "            \"title\": \"SpeakerIdentification\",\n",
    "            \"description\": \"Identification of speakers in an interview transcript\",\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"interviewer\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Label for the interviewer (speaker0 or speaker1)\"\n",
    "                },\n",
    "                \"interviewee\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Label for the interviewee (speaker0 or speaker1)\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"interviewer\", \"interviewee\"]\n",
    "        }\n",
    "        \n",
    "        # Create messages for speaker identification\n",
    "        messages = [\n",
    "            SystemMessage(content=\"\"\"\n",
    "                You are an AI assistant specialized in analyzing technical interview transcripts.\n",
    "                \n",
    "                Your task is to identify which speaker is the interviewer and which is the interviewee in the provided transcript.\n",
    "                \n",
    "                The interviewer typically:\n",
    "                - Asks most of the questions\n",
    "                - Guides the conversation\n",
    "                - Introduces coding problems or technical concepts\n",
    "                - Evaluates responses\n",
    "                \n",
    "                The interviewee typically:\n",
    "                - Answers questions\n",
    "                - Explains their reasoning\n",
    "                - Provides solutions to coding problems\n",
    "                - Demonstrates technical knowledge\n",
    "                \n",
    "                Return your analysis with the labels \"interviewer\" and \"interviewee\" assigned to either \"speaker0\" or \"speaker1\".\n",
    "            \"\"\"),\n",
    "            HumanMessage(content=f\"Here is the interview transcript:\\n\\n{transcript}\")\n",
    "        ]\n",
    "        \n",
    "        # Use structured output with our speaker schema\n",
    "        structured_llm = model.with_structured_output(speaker_schema)\n",
    "        \n",
    "        # Get response and extract speaker roles\n",
    "        try:\n",
    "            response = await structured_llm.ainvoke(messages)\n",
    "            print(f\"Identified speakers: Interviewer={response['interviewer']}, Interviewee={response['interviewee']}\")\n",
    "            \n",
    "            # Simply return the speaker roles - LangGraph will handle state updates\n",
    "            return {\"speaker_identification\": response}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error identifying speakers: {e}\")\n",
    "            return {\"error\": f\"Error identifying speakers: {e}\"}\n",
    "            \n",
    "    except AttributeError as e:\n",
    "        print(f\"Error accessing transcript: {e}\")\n",
    "        return {\"error\": \"Could not access transcript for speaker identification\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "fcd13f14-72b1-4f80-82b8-6598a39f925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def extract_questions(state: Evaluation):\n",
    "    print(\"Extracting detailed question-answer pairs from interview transcript\")\n",
    "    try:\n",
    "        transcript = state.video.attributes.transcript\n",
    "        \n",
    "        if not transcript or not transcript.strip():\n",
    "            return {\"error\": \"No transcript\", \"qa_pairs\": []}\n",
    "        \n",
    "        # Get speaker identification if available\n",
    "        interviewer = getattr(state, 'speaker_identification', {}).get('interviewer')\n",
    "        interviewee = getattr(state, 'speaker_identification', {}).get('interviewee')\n",
    "        \n",
    "        # Using Pydantic v2 approach with RootModel\n",
    "        from pydantic import RootModel\n",
    "        # Ensure List is imported here\n",
    "        from typing import List\n",
    "        QAPairsList = RootModel[List[QuestionAnswerPair]]\n",
    "        \n",
    "        # Build detailed prompt with instructions\n",
    "        system_message = f\"\"\"\n",
    "        You are analyzing a technical interview transcript to extract detailed question-answer pairs.\n",
    "        \n",
    "        Guidelines for extraction:\n",
    "        1. Identify complete technical questions that test knowledge\n",
    "        2. Capture the full context of both questions and answers\n",
    "        3. Include any code examples mentioned in questions or answers\n",
    "        4. Recognize multi-part questions and answers that span multiple turns\n",
    "        5. Focus only on substantial technical questions, not conversational remarks\n",
    "        \n",
    "        In this transcript:\n",
    "        - The interviewer is labeled as {interviewer}\n",
    "        - The interviewee is labeled as {interviewee}\n",
    "        \n",
    "        Extract at least 5 substantial technical questions from the interviewer and full answers from the interviewee.\n",
    "        \"\"\"\n",
    "        \n",
    "        human_message = f\"Here is the interview transcript to analyze:\\n\\n{transcript}\"\n",
    "        \n",
    "        messages = [\n",
    "            SystemMessage(content=system_message),\n",
    "            HumanMessage(content=human_message)\n",
    "        ]\n",
    "        \n",
    "        # Alternative approach using messages without structured output\n",
    "        response = await model.ainvoke(messages)\n",
    "        \n",
    "        # Parse the response using a more robust approach\n",
    "        import json\n",
    "        import re\n",
    "        \n",
    "        # Process the text response to extract json-like content\n",
    "        # This is a fallback approach since structured output failed\n",
    "        qa_pairs: List[QuestionAnswerPair] = []\n",
    "        \n",
    "        # Use a direct prompt that requests specific formatting\n",
    "        extraction_prompt = f\"\"\"\n",
    "        Analyze this technical interview and extract exactly 5-7 question-answer pairs.\n",
    "        The interviewer is {interviewer} and the interviewee is {interviewee}.\n",
    "        \n",
    "        For each pair, format your response as:\n",
    "        \n",
    "        QUESTION: [Full question text]\n",
    "        ANSWER: [Full answer text]\n",
    "        TOPIC: [Technical topic]\n",
    "        \n",
    "        Make sure to capture multi-part questions and their complete answers.\n",
    "        Include relevant code examples in both questions and answers.\n",
    "        Focus on substantial technical questions, not conversational remarks.\n",
    "        \n",
    "        Transcript:\n",
    "        {transcript}\n",
    "        \"\"\"\n",
    "        \n",
    "        extraction_response = await model.ainvoke(extraction_prompt)\n",
    "        \n",
    "        # Process the formatted response\n",
    "        response_text = extraction_response.content\n",
    "        qa_blocks = re.split(r'QUESTION:', response_text)[1:]  # Skip the first empty element\n",
    "        \n",
    "        for block in qa_blocks:\n",
    "            try:\n",
    "                question_text = block.split('ANSWER:')[0].strip()\n",
    "                remaining = block.split('ANSWER:')[1]\n",
    "                \n",
    "                # Handle if TOPIC is present\n",
    "                if 'TOPIC:' in remaining:\n",
    "                    answer_text = remaining.split('TOPIC:')[0].strip()\n",
    "                    # We don't need to store topic but could add it if QuestionAnswerPair is updated\n",
    "                else:\n",
    "                    answer_text = remaining.strip()\n",
    "                \n",
    "                qa_pairs.append(QuestionAnswerPair(\n",
    "                    question_text=question_text,\n",
    "                    answer_text=answer_text\n",
    "                ))\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing a Q&A block: {e}\")\n",
    "        \n",
    "        print(f\"Extracted {len(qa_pairs)} detailed question-answer pairs\")\n",
    "        return {\"qa_pairs\": qa_pairs}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting question-answer pairs: {e}\")\n",
    "        return {\"error\": f\"Error extracting question-answer pairs: {e}\", \"qa_pairs\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "914c7c1a-0c8e-4451-b189-7f76d2f51893",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def grade_answer(qa_pair: QuestionAnswerPair) -> QuestionAnswerPair:\n",
    "    \"\"\"Grade a single question-answer pair.\"\"\"\n",
    "    \n",
    "    # Create a schema for structured output\n",
    "    grading_schema = {\n",
    "        \"title\": \"GradingDetails\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"score\": {\n",
    "                \"type\": \"integer\",\n",
    "                \"description\": \"Score on scale of 0-100 based on technical accuracy, completeness, and clarity\",\n",
    "                \"minimum\": 0,\n",
    "                \"maximum\": 100\n",
    "            },\n",
    "            \"rating\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"Strong Yes\", \"Yes\", \"No\", \"Strong No\"],\n",
    "                \"description\": \"Overall hiring recommendation based on this answer\"\n",
    "            },\n",
    "            \"key_facts\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\"type\": \"string\"},\n",
    "                \"description\": \"List of key technical facts or concepts demonstrated in the answer\"\n",
    "            },\n",
    "            \"strengths\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\"type\": \"string\"},\n",
    "                \"description\": \"List of strengths in the answer\"\n",
    "            },\n",
    "            \"weaknesses\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\"type\": \"string\"},\n",
    "                \"description\": \"List of weaknesses or areas for improvement in the answer\"\n",
    "            },\n",
    "            \"justification\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Brief justification for the score and rating\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"score\", \"rating\", \"key_facts\", \"strengths\", \"weaknesses\", \"justification\"]\n",
    "    }\n",
    "    \n",
    "    # Create a structured output model\n",
    "    structured_model = model.with_structured_output(grading_schema)\n",
    "    \n",
    "    # Construct the grading prompt\n",
    "    system_prompt = \"\"\"You are an expert technical interviewer for JavaScript/TypeScript engineering roles.\n",
    "    Your task is to grade the candidate's answer to a technical interview question. \n",
    "    \n",
    "    Evaluation criteria:\n",
    "    - Technical accuracy (40%): Is the answer technically correct and demonstrate understanding?\n",
    "    - Completeness (30%): Does the answer address all parts of the question thoroughly?\n",
    "    - Clarity & communication (30%): Is the answer well-structured, clear, and easy to follow?\n",
    "    \n",
    "    Scoring guide:\n",
    "    - 90-100: Exceptional answer that demonstrates deep expertise\n",
    "    - 75-89: Strong answer with minor areas for improvement\n",
    "    - 60-74: Acceptable answer with some gaps or misconceptions\n",
    "    - 40-59: Weak answer with significant gaps or errors\n",
    "    - 0-39: Very poor answer showing lack of understanding\n",
    "    \n",
    "    Rating guide:\n",
    "    - Strong Yes: Exceptional answer, candidate demonstrates mastery\n",
    "    - Yes: Good answer, candidate shows competence\n",
    "    - No: Problematic answer, candidate needs improvement\n",
    "    - Strong No: Poor answer, candidate lacks fundamental understanding\n",
    "    \n",
    "    Provide a structured evaluation with a numerical score, qualitative rating, key facts demonstrated, \n",
    "    strengths, weaknesses, and brief justification.\"\"\"\n",
    "    \n",
    "    human_prompt = f\"\"\"Question: {qa_pair.question_text}\n",
    "    \n",
    "    Candidate's Answer: {qa_pair.answer_text}\n",
    "    \n",
    "    Please evaluate this answer according to the criteria.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=human_prompt)\n",
    "    ]\n",
    "    \n",
    "    # Get the grading result\n",
    "    grading_result = await structured_model.ainvoke(messages)\n",
    "    \n",
    "    # Update the qa_pair with the score and rating\n",
    "    qa_pair.score = grading_result[\"score\"]\n",
    "    qa_pair.rating = grading_result[\"rating\"]\n",
    "    \n",
    "    # Create a GradingDetails object and assign it to grading_details\n",
    "    details = GradingDetails(\n",
    "        score=grading_result[\"score\"],\n",
    "        rating=grading_result[\"rating\"],\n",
    "        key_facts=grading_result[\"key_facts\"],\n",
    "        strengths=grading_result[\"strengths\"],\n",
    "        weaknesses=grading_result[\"weaknesses\"],\n",
    "        justification=grading_result[\"justification\"]\n",
    "    )\n",
    "    qa_pair.grading_details = details\n",
    "    \n",
    "    return qa_pair\n",
    "\n",
    "async def parallel_grade_answers(qa_pairs: List[QuestionAnswerPair]) -> List[QuestionAnswerPair]:\n",
    "    \"\"\"Grade multiple question-answer pairs in parallel.\"\"\"\n",
    "    \n",
    "    # Create tasks for parallel execution\n",
    "    tasks = [grade_answer(qa_pair) for qa_pair in qa_pairs]\n",
    "    \n",
    "    # Execute all grading tasks in parallel\n",
    "    graded_pairs = await asyncio.gather(*tasks)\n",
    "    \n",
    "    return graded_pairs\n",
    "\n",
    "async def grade_interview_answers(state: Evaluation):\n",
    "    \"\"\"Grade all question-answer pairs in parallel and calculate an overall score.\"\"\"\n",
    "    print(\"Grading interview answers in parallel...\")\n",
    "    \n",
    "    if not state.qa_pairs or len(state.qa_pairs) == 0:\n",
    "        return {\"error\": \"No question-answer pairs to grade\"}\n",
    "    \n",
    "    # Grade all question-answer pairs in parallel\n",
    "    graded_pairs = await parallel_grade_answers(state.qa_pairs)\n",
    "    \n",
    "    # Calculate overall score (average of all answer scores)\n",
    "    if graded_pairs:\n",
    "        total_score = sum(pair.score for pair in graded_pairs if pair.score is not None)\n",
    "        overall_score = total_score // len(graded_pairs)\n",
    "    else:\n",
    "        overall_score = 0\n",
    "    \n",
    "    print(f\"Grading complete. Overall interview score: {overall_score}/100\")\n",
    "    \n",
    "    # Return updated state with graded pairs and overall score\n",
    "    return {\"qa_pairs\": graded_pairs, \"score\": overall_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "fd16bfc6-b0a6-44a0-a542-8e26f622aed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the graph structure\n",
    "workflow = StateGraph(Evaluation)\n",
    "\n",
    "# Add all nodes\n",
    "workflow.add_node(\"get_interview_state\", get_interview_state)\n",
    "workflow.add_node(\"identify_speakers\", identify_speakers)\n",
    "workflow.add_node(\"extract_questions\", extract_questions)\n",
    "workflow.add_node(\"grade_interview_answers\", grade_interview_answers)\n",
    "\n",
    "# Define the edges for the workflow\n",
    "workflow.add_edge(START, \"get_interview_state\")\n",
    "workflow.add_edge(\"get_interview_state\", \"identify_speakers\")\n",
    "workflow.add_edge(\"identify_speakers\", \"extract_questions\")\n",
    "workflow.add_edge(\"extract_questions\", \"grade_interview_answers\")\n",
    "workflow.add_edge(\"grade_interview_answers\", END)\n",
    "\n",
    "# Compile the graph with memory handling\n",
    "memory = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# Function to run the graph for a given interview URL\n",
    "async def process_interview(url: str):\n",
    "    # Create initial state with URL\n",
    "    initial_state = Evaluation(url=url)\n",
    "    \n",
    "    # Run the graph\n",
    "    config = {\"configurable\": {\"thread_id\": str(datetime.now().timestamp())}, \"callbacks\": [langfuse_handler]}\n",
    "    result = await graph.ainvoke(initial_state, config)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "163f1d83-627f-499f-b887-aac27ea1a8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing speakers from video data\n",
      "Identified speakers: Interviewer=speaker0, Interviewee=speaker1\n",
      "Extracting detailed question-answer pairs from interview transcript\n",
      "Extracted 5 detailed question-answer pairs\n",
      "Grading interview answers in parallel...\n",
      "Grading complete. Overall interview score: 85/100\n"
     ]
    }
   ],
   "source": [
    "# Example execution\n",
    "# Replace with your actual interview URL\n",
    "interview_url = \"https://core.g2i.co/api/v2/videos/250c4ef3-114a-4709-8fdc-3419d48f8908\"\n",
    "\n",
    "# Run the interview processing workflow\n",
    "try:\n",
    "    result = await process_interview(interview_url)\n",
    "    \n",
    "    # Print the interview summary\n",
    "    if hasattr(result, 'error') and result.error:\n",
    "        print(f\"Error processing interview: {result.error}\")\n",
    "    else:\n",
    "        pass\n",
    "        # print_interview_summary(result)\n",
    "except Exception as e:\n",
    "    print(f\"Error running interview workflow: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32b759c-4817-436f-abb3-8f00ccc7621f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
